{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs in Keras\n",
    "\n",
    "Let’s build a simple CNN for image classification. In this example, we’ll use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset—a common benchmark dataset for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cifar10\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize image pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some of the CIFAR-10 images along with their class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "num_images = 25\n",
    "\n",
    "# Randomly select indices from the training set\n",
    "random_indices = np.random.choice(x_train.shape[0], num_images, replace=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[idx])\n",
    "    plt.xlabel(class_names[np.argmax(y_train[idx])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check on Class Balance\n",
    "\n",
    "It is important to understand the distribution of labels in our data to observe whether we have class imbalance issues. In the following cell we check the class balance for both the training and testing sets. This example first converts one-hot encoded labels back to class indices (since we had converted the labels to one hot encoded vectors above), then calculates the number of samples per class using NumPy's bincount. Additionally, it provides a bar chart visualization of the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(y_train.shape) > 1 and y_train.shape[1] > 1:\n",
    "    y_train_labels = np.argmax(y_train, axis=1)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_train_labels = y_train.flatten()\n",
    "    y_test_labels = y_test.flatten()\n",
    "\n",
    "# Calculate class counts using np.bincount\n",
    "train_class_counts = np.bincount(y_train_labels)\n",
    "test_class_counts = np.bincount(y_test_labels)\n",
    "\n",
    "# Print out class distribution for training and testing sets\n",
    "print(\"Training set class distribution:\")\n",
    "for i, count in enumerate(train_class_counts):\n",
    "    print(f\"Class {i}: {count} samples\")\n",
    "\n",
    "print(\"\\nTesting set class distribution:\")\n",
    "for i, count in enumerate(test_class_counts):\n",
    "    print(f\"Class {i}: {count} samples\")\n",
    "\n",
    "# Visualize the class distribution using bar charts\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(np.arange(len(train_class_counts)), train_class_counts, tick_label=class_names)\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(np.arange(len(test_class_counts)), test_class_counts, tick_label=class_names)\n",
    "plt.title('Testing Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model Architecture\n",
    "\n",
    "Here, we construct a sequential model with multiple convolutional and pooling layers. Later, we flatten the output and pass it to dense layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First convolutional block\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Second convolutional block\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten the output and add fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and Training the Model\n",
    "\n",
    "We compile the model with an appropriate optimizer, loss function, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the `.fit()` method, specifying the number of epochs and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.2,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the training convergence history, including both loss and accuracy for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance\n",
    "\n",
    "After training, we evaluate the model on the test set to see how well it generalizes to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation can help improve model robustness by artificially enlarging the dataset through random transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Use the generator in model training\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                    epochs=30,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling\n",
    "\n",
    "While adaptive learning rate algorithms (such as Adam) adapt learning rates on a per-parameter basis, externally scheduling the global learning rate can further refine and stabilize training for better performance. Below is an example of how we can use Keras's LearningRateScheduler callback to dynamically adjust the learning rate during training. In this example, we reduce the learning rate by 10% every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    # Decrease the learning rate by 10% every 5 epochs\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        return lr * 0.9\n",
    "    return lr\n",
    "\n",
    "# Create the LearningRateScheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Use the scheduler during training\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                    epochs=30,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some situations when it might be beneficial to use a learning rate scheduler:\n",
    "- If you notice that your training or validation loss stops improving (or even worsens) after several epochs, this might be a sign that your learning rate is too high for fine-tuning in later stages.\n",
    "- When the loss fluctuates widely without clear progress, lowering the learning rate can help stabilize updates.\n",
    "- When fine-tuning a model on a new dataset, starting with a higher learning rate and then gradually reducing it can help adjust the pre-trained weights without causing drastic updates.\n",
    "- Often, the decision comes down to experimenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining both learning rate scheduling and early stopping\n",
    "\n",
    "We can also combine learning rate scheduling with an early stopping callback. The learning rate scheduler adjusts the learning rate to help the optimizer fine-tune the weights over time, while early stopping monitors a chosen metric (e.g., validation loss) to halt training if the model stops improving.\n",
    "\n",
    "Below is an example that demonstrates how to use both callbacks together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Use both callbacks during training\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                    epochs=30,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[lr_scheduler, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.argmax(y_test, axis=1)\n",
    "predicted_labels = np.argmax(model.predict(x_test), axis=1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing (Mis)Predictions\n",
    "\n",
    "It is important to check samples on which the model is predicting correctly and also observe mispredictions to see if we can gain some insight of where the model is performing well and were it is failing. In the following example, we randomly select samples from the testing set, make predictions using the trained model, and then visualize the images with both the predicted and ground truth labels. If the prediction is correct, the label text is shown in green; if not, it’s shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Set number of random samples to display\n",
    "num_samples = 25\n",
    "random_indices = np.random.choice(len(x_test), num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[idx])\n",
    "    \n",
    "    # Retrieve predicted and true label names\n",
    "    pred_label = class_names[predicted_labels[idx]]\n",
    "    true_label = class_names[true_labels[idx]]\n",
    "    \n",
    "    # Set label color: green if prediction is correct, red otherwise\n",
    "    color = 'green' if predicted_labels[idx] == true_labels[idx] else 'red'\n",
    "    \n",
    "    plt.xlabel(f\"P: {pred_label}\\nT: {true_label}\", color=color)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize learned filters & produced feature maps\n",
    "\n",
    "Now we visualize some learned filters in each layer and some produced feature maps. The following examples iterates through each convolutional layer in the model, extracts the learned filters, and visualizes a few random ones for each layer. The code normalizes the filter values for display and handles both single-channel (grayscale) and three-channel (RGB) filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    # Check if the layer is a Conv2D layer\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        filters, biases = layer.get_weights()\n",
    "        # filters shape: (filter_height, filter_width, in_channels, out_channels)\n",
    "        num_filters = filters.shape[-1]\n",
    "        print(f\"Visualizing filters from layer: {layer.name} with {num_filters} filters\")\n",
    "\n",
    "        # Select a few random filters \n",
    "        num_to_show = min(6, num_filters)\n",
    "        random_filter_indices = np.random.choice(num_filters, num_to_show, replace=False)\n",
    "\n",
    "        # Create subplots for the selected filters\n",
    "        n_cols = 3\n",
    "        n_rows = int(np.ceil(num_to_show / n_cols))\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "        axes = axes.flatten()  # flatten in case of multiple rows\n",
    "\n",
    "        for i, filter_index in enumerate(random_filter_indices):\n",
    "            # Get the filter weights: shape (filter_height, filter_width, in_channels)\n",
    "            filt = filters[:, :, :, filter_index]\n",
    "\n",
    "            # Normalize filter values to 0-1 for visualization\n",
    "            f_min, f_max = filt.min(), filt.max()\n",
    "            filt_norm = (filt - f_min) / (f_max - f_min + 1e-5)  # avoid division by zero\n",
    "            ax = axes[i]\n",
    "            # If the filter has 3 channels, display it as an RGB image; otherwise as grayscale.\n",
    "            if filt_norm.shape[-1] == 3:\n",
    "                ax.imshow(filt_norm)\n",
    "            else:\n",
    "                # For single-channel filters, remove the channel dimension and use a grayscale colormap.\n",
    "                ax.imshow(filt_norm[:, :, 0], cmap='gray')\n",
    "            ax.set_title(f'Filter {filter_index}')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Hide any extra subplots\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.suptitle(f\"Filters from layer: {layer.name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize some feature maps corresponding to each layer for a randomly selected testing sample. We select a random test image, compute the intermediate feature maps for each convolutional layer, and visualizes a few randomly chosen feature maps from each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: the following cell runs correctly only with tf version of < 2.15. It will fail on Rosie due to a bug in Keras version 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.randint(0, x_test.shape[0])\n",
    "sample_image = x_test[sample_index]\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(sample_image)\n",
    "plt.title(\"Original Test Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "sample_image_expanded = np.expand_dims(sample_image, axis=0)  # add batch dimension\n",
    "\n",
    "# Iterate over the layers in the model\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        # Create a model that outputs the activations of the current layer\n",
    "        intermediate_model = tf.keras.Model(inputs=model.input, outputs=layer.output)\n",
    "        \n",
    "        intermediate_model = tf.keras.Model(inputs=model.input, outputs=layer.output)\n",
    "        \n",
    "        feature_maps = intermediate_model.predict(sample_image_expanded)\n",
    "        # feature_maps shape: (1, height, width, channels)\n",
    "        num_feature_maps = feature_maps.shape[-1]\n",
    "        print(f\"Visualizing feature maps from layer: {layer.name} with {num_feature_maps} feature maps\")\n",
    "        \n",
    "        # Randomly select a subset of feature maps to display\n",
    "        num_to_show = min(6, num_feature_maps)\n",
    "        random_indices = np.random.choice(num_feature_maps, num_to_show, replace=False)\n",
    "        \n",
    "        n_cols = 3\n",
    "        n_rows = int(np.ceil(num_to_show / n_cols))\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, fmap_index in enumerate(random_indices):\n",
    "            fmap = feature_maps[0, :, :, fmap_index]\n",
    "            # Normalize the feature map to [0, 1] for visualization\n",
    "            fmap_norm = (fmap - fmap.min()) / (fmap.max() - fmap.min() + 1e-5)\n",
    "            axes[i].imshow(fmap_norm, cmap='viridis')\n",
    "            axes[i].set_title(f\"Map {fmap_index}\")\n",
    "            axes[i].axis(\"off\")\n",
    "        \n",
    "        # Hide any unused subplots if necessary\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "        \n",
    "        plt.suptitle(f\"Feature Maps from layer: {layer.name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with a DNN model\n",
    "\n",
    "Now we apply a dense (fully connected) neural network architecture and compare its performance with CNN. Unlike a CNN that leverages spatial structure via convolutional layers, this dense model first flattens the image and then uses several Dense layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense = Sequential()\n",
    "model_dense.add(Flatten(input_shape=x_train.shape[1:]))    # Flatten the image from (32, 32, 3) to a 1D vector of 3072 elements\n",
    "model_dense.add(Dense(512, activation='relu'))     # First hidden layer with 512 units and ReLU activation\n",
    "# model_dense.add(Dropout(0.5))  # Dropout for regularization\n",
    "model_dense.add(Dense(256, activation='relu'))     # Second hidden layer with 256 units and ReLU activation\n",
    "# model_dense.add(Dropout(0.5))\n",
    "model_dense.add(Dense(num_classes, activation='softmax'))      # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the dense model using the Adam optimizer and categorical crossentropy loss\n",
    "model_dense.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train the dense model\n",
    "history_dense = model_dense.fit(x_train, y_train,\n",
    "                                epochs=30,\n",
    "                                batch_size=64,\n",
    "                                validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_dense.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Dense model test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history_dense.history['loss'], label='Train Loss')\n",
    "plt.plot(history_dense.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dense.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_dense.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Tips \n",
    "\n",
    "- Experiment with architectures \n",
    "    - There’s no one-size-fits-all model. Experiment with the number of layers, filter sizes, and hyperparameters.\n",
    "- Monitor training\n",
    "    - Use callbacks to track training progress and avoid overfitting.\n",
    "- Preprocessing matters\n",
    "    - Ensure your data is properly normalized and augmented where appropriate.\n",
    "- Leverage transfer learning\n",
    "    - Pre-trained models can significantly boost performance, especially with limited data.\n",
    "- Document and iterate\n",
    "    - Keep detailed records of experiments and adjustments to refine your model over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
